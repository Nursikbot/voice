# Voice AI Research for Vk_lab
## Содержание
1) Основной файл: audio_mood.ipynb — содержит обученную модель Wav2Vec 2.0 и анализ результатов, включая сравнение с базовой моделью и анализ сигналов. Также выгружены результаты классификации.
2) Результаты на тестовой выборке: файл output.txt с данными в формате: file_id | 01 | 02 | 03 | 04 | 05 | 06 | 07 | 08
3) Файл с признаками: выгружен в features.csv (eGeMAPSv02). Важно загрузить его, как указано в коде, либо использовать для дальнейшей обработки данных.

## Проект содержит ссылки на загрузку датасетов и обученной модели::
1) RAVDESS -> https://drive.google.com/file/d/1gn79SrzmBDclSP6q75UUs2hZ42cIeqkf/view?usp=sharing
2) Тестовый датасет -> https://drive.google.com/file/d/1--3dUqmNRJyhj2Ht1gl9ktoRaafbuHhw/view?usp=sharing
3) Веса и состояние оптимизатора модели (.pth) -> https://drive.google.com/file/d/1ZZPWhGp5fd3VrtBNI4EXo8QF5ogQIUy4/view?usp=sharing

### Плюсы Wav2Vec 2.0:
1) Извлечение признаков: Энкодер модели выделяет признаки из аудиосигнала без привязки к языку или транскрипции. Он сжимает сигнал до вектора фиксированной длины, обучаясь предсказывать его части по аналогии с методами NLP, где модель восстанавливает замаскированные участки.
2) Многофункциональность: Предобученная Wav2Vec 2.0 подходит для задач распознавания речи, классификации аудиосигналов и извлечения текста с высокой точностью.
3) Интеграция с текстовыми классификаторами: Можно использовать модель для перевода аудио в текст, а затем применять готовые классификаторы для дальнейшего анализа (например, анализ тональности).

### Минусы Wav2Vec 2.0:
1) Ресурсоемкость: Тонкая настройка модели 'jonatasgrosman/wav2vec2-large-xlsr-53-english' требует значительных вычислительных ресурсов, особенно для настройки гиперпараметров. Также требуется учитывать архитектуру модели при замораживании и размораживании слоев.
2) Зависимость от объема данных: Для достижения высокого качества необходимо дообучение на большом количестве данных. При уменьшении объема тренировочного набора наблюдается значительное падение качества модели.
   
## Особенность выбора wav2vec2 для задачи классификации эмоций
Модель Wav2Vec 2.0 удобна благодаря встроенному автоэнкодеру, который генерирует взвешенные признаки для дальнейшего анализа. При классификации эмоций важно учитывать такие характеристики, как гармоники, мел-спектрограммы, хрома, MFCC и контраст спектра.
Ссылки на использованные ресурсы:
1) Обучающий плейлист: https://youtube.com/playlist?list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&si=EhmZAE-tJuD9PQos
2) Стаья по распознаванию эмоций на греческом языке: https://github.com/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb?ysclid=lnu8h22nnc158063470
3) Работа с базовой моделью и анализом сигналов была мотивирована работой: https://www.kaggle.com/code/ejlok1/audio-emotion-part-3-baseline-model/notebook
4) Предобученная модель для перевода аудио в текст: https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english
5) Cсылка на Wav2Vec2: https://huggingface.co/docs/transformers/model_doc/wav2vec2
6) Работа с использованием CNN: https://github.com/huaiyukhaw/speech-emotion-recognition/blob/master/CNN.ipynb

## Особенности RAVDESS в файле(audio_speech_actors_01-24) и разделения train\test
* Датасет включает две фразы, исполненные 24 актерами (мужчинами и женщинами) с различными тембрами и высотой голоса.
* Каждая фраза повторяется с изменением тона. Использование аугментации сигнала (например, добавление шумов) помогает улучшить обучение нейросети(как, например делалось в работе -> https://www.kaggle.com/code/pranavbelhekar/audio-emotion-classification-part-3 ).
* Всего в датасете 60 аудиозаписей для каждого актера. Эмоции распределены следующим образом: 4 записи для нейтрального состояния, 8 — для остальных эмоций, что в сумме дает 96 записей для нейтрального состояния и по 192 для других эмоций.
* Сигналы были выровнены до одинаковой длины (60000).
* Разделение на тренировочные и тестовые данные проводилось случайным образом (80/20) с использованием random_split, что помогает избежать переобучения на конкретных актерах.
* Можно визульно разделить яркие эмоции от пассивных на основе анализа mcfc(The Mel-frequency cepstral coefficients )или энергии(амплитуды^2 сигналов)

### График функций потерь и accuracy на train и test
<figure>
  <img
  src="train_val_loss_accuracy.png"
  >
</figure>    

### Таблицы с метриками  
<figure>
  <img
  src="reports.bmp"
  >
  <figcaption>Таблица 1 - Метрики, посчитанные на val_dataset</figcaption>
</figure>  
Функция потерь на валидационной выборке начинает расти, причем классы распределены не равномерно(neutral вдвое реже), важно обратить внимание на f1 метрику

Результат валидации хорошо виден тут:

### Confusion matrix on validation
<figure>
  <img
  src="confusion_matrix.png"
  >
</figure>    
Низкая точность предсказания на neutral ожидаема(данных меньше), а вот от 'angry ' и 'hаppy' как от более ампитудно-выделенных сигналов, ожидалось точность выше. Если анализировать mcfc карту глубины, усреднив данные по оси времени, действительно, визульно тяжело различитить различающие особенности этих двух эмоций:

### Signal processing
<figure>
  <img
  src="signal_process.png"
  >
</figure>   

Обученную модель прогоним на тестовой выборке и посмотрим результат предсказаний(преобладание тех или иных эмоций)

### Results on test dataset using wav2vec
<figure>
  <img
  src="emotionswav2vec2.png"
  >
</figure>  

Сравнение с базовой моделью(RandomForest), отбор функциональные признаков был совершен с помощью библиотеки asmile с небольшой моделью eGeMAPSv02 (88 функциональных признаков)

### Таблицы с метриками  
<figure>
  <img
  src="reports_base.bmp"
  >
  <figcaption>Таблица 2 - Метрики, посчитанные на val_dataset</figcaption>
</figure>  

### Results on test dataset using RandomForest
<figure>
  <img
  src="emotionsRandomForest.png"
  >
</figure>  

Начата попытка разобраться с тонкостями обработки сигналов

### Обсуждение, идеи
* Модель 'jonatasgrosman/wav2vec2-large-xlsr-53-english' показала хорошие результаты на RAVDESS, однако требуется дополнительная настройка токенов (sep_token, cls_token, mask_token). Для борьбы с переобучением можно увеличить размер батчей и заморозить входные слои на начальных этапах обучения.
* Для базовой модели стоит рассмотреть более широкий спектр признаков, например, использовать модель ComParE_2016. Анализ актуальности признаков для классификации эмоций также может улучшить результаты.
* Также необходимо обучить модель на других датасетах (SAVEE, TESS), учитывая разнообразие диалектов и интонаций.
* Рассмотреть использование ансамбля моделей с применением взвешенного голосования для улучшения результатов классификации.


